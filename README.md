# Streamlined-Data-Medallion-Architecture-for-Enhanced-Analytics

## ðŸ“œ Table of Contents
- [ProjectOverview](#project-overview-)
- [Medallion Architecture](#what-is-the-medallion-architecture-)
- [E-commerce database](#e-commerce-data-store)
- [Building Analytical Platform](#building-the-analytics-platform)
- [New EventStream](#create-a-new-event-stream)
- [Generating Stream Data](#using-notebook-to-generate-the-stream-data)
- [Destination Config](#define-destination-in-event-stream)
- [Build KQL DB Schema](#build-the-kql-db-schema)
- [Data Pipeline](#data-pipeline)
- [Gold Layers Views](#materialized-view)
- [Real Time Dashboard](#real---time-dashboard)

# Project Overview : 

To build a medallion architecture in Microsoft Fabric for real-time analytics, start by creating data pipelines to copy data from a sample data. Next, ingest streaming events into Microsoft Fabric's Real-Time Analytics (RTA) with EventStream for immediate processing. Implement necessary data transformations within RTA to prepare the data for analysis, and then develop interactive reports and dashboards to visualize real-time insights. Finally, continuously monitor and optimize the performance of your pipelines and dashboards to ensure efficient data flow and responsiveness, enabling informed decision-making based on real-time analytics.

# What is the Medallion Architecture :

Medallion architecture, also known as multi-hop architecture, is a data design pattern that organizes data within a lakehouse to enhance its quality progressively through three distinct layers: Bronze, Silver, and Gold. The Bronze layer contains raw, unprocessed data, while the Silver layer involves filtering and cleansing to improve usability. Finally, the Gold layer features enriched and aggregated data ready for analysis and business applications. This structured approach allows organizations to incrementally refine their data, ensuring higher quality and better alignment with business needs as it flows through each layer, ultimately facilitating effective decision-making and insights.

## Bronze Layer 
- Contains raw data, sometimes referenced as the data staging area.
- Not accessible to consumers only to engineers.
- May contain private or personal information.

## Silver Layer
- Contains the deduplicated data and accessible to all consumers
- This data can be used by Data Analyst , Data Engineers and Data Scientist according to their needs

## Gold Layer
- Contains aggregated data
- Final analytical data use for building the dashboards

# E-commerce data store 

E - commerce database consist of the 
 - Customer
 - Address
 - SalesOrderHeader
 - SalesOrderDetail
 - Event : a click or impression event

# Building the Analytics Platform

### Create a Fabric Workspace and in that create a New EventHouse

In the image below, we are establishing an event house to manage all our batch and streaming data. This data will then be utilized in a lakehouse for further transformation based on our requirements.

![AltImage](https://github.com/Jignesh3006/Streamlined-Data-Medallion-Architecture-for-Enhanced-Analytics/blob/main/Images/Step%20-%201.png)

In the image below Enabling data availability of KQL Database means in OneLake means that the customers can enjoy the best of both worlds they can query the data with high performance and low latency in their KQL database and query the same data in Delta Lake format via any other Fabric engines such as Power BI Direct Lake mode, Warehouse, Lakehouse, Notebooks, and more.KQL Database offers a robust mechanism to batch the incoming streams of data into one or more Parquet files suitable for analysis. The Delta Lake representation is provided to keep the data open and reusable. This logical copy is managed once, is paid for once and users should consider it a single data set.

![AltImage](https://github.com/Jignesh3006/Streamlined-Data-Medallion-Architecture-for-Enhanced-Analytics/blob/main/Images/Step%20-%202.png)

Create a new Lakehouse and we can see all the tables present in the KQL Database and can see in the Lakehouse 

![AltImage](https://github.com/Jignesh3006/Streamlined-Data-Medallion-Architecture-for-Enhanced-Analytics/blob/main/Images/Step%20-3.png)

By creating the shortcut we can see the all the tables present in the KQL DB that can reflect in the Lakehouse

![AltImage](https://github.com/Jignesh3006/Streamlined-Data-Medallion-Architecture-for-Enhanced-Analytics/blob/main/Images/Step-new.png)

# Create a new Event Stream

In the below image we will be creating a new eventstream which will take the impressions and click events generated by notebook. The events will be streamed into an eventstream and consumed in the KQL DB.

![AltImage](https://github.com/Jignesh3006/Streamlined-Data-Medallion-Architecture-for-Enhanced-Analytics/blob/main/Images/Step%20-%204.png)

When we create a **Custom app** as a source , an event hub is created and connected to Eventstream for us.
Click on the eventstream source - Custom App to get the event hub endpoint and keys send the events from the notebook. 

![AltImage](https://github.com/Jignesh3006/Streamlined-Data-Medallion-Architecture-for-Enhanced-Analytics/blob/main/Images/Step%20-%205.png)

Below click on sample code and copy the connectionString to a notepad and also get the keys for the Event-hub

![AltImage](https://github.com/Jignesh3006/Streamlined-Data-Medallion-Architecture-for-Enhanced-Analytics/blob/main/Images/Step%20-%206.png)

# Using Notebook to generate the Stream Data

## Create the Environment
 In order to compute to run the notebooks to have the right libraries we will create an **environment**. Add the following libraries in the environment
  - azure - eventhub
  - faker
  - pyodbc
![AltImage](https://github.com/Jignesh3006/Streamlined-Data-Medallion-Architecture-for-Enhanced-Analytics/blob/main/Images/Step%20-%207.png)

## Run the notebook 

Below is the code to create the streaming events which will create the dummy data for the **Clicks** and the **Impression** 

``` python
import json
from azure.eventhub import EventHubProducerClient, EventData
import os
import socket
import random

from random import randrange


eventHubConnString = "Endpoint=sb://esehdewcdlgy5d4714h4zbya.servicebus.windows.net/;SharedAccessKeyName=key_8xxxxxxxx262-19b2-7771-c9fe2bc0f021;SharedAccessKey=RGTy2Nx0cgY6jxxxxxxxaLg/flFaJ+AEhGJ0uCE=;EntityPath=es_c8954a72-1355-40b1-8a9a-b4cb9f53ebb4" 
eventHubNameevents = "es_c8954a72-1355-40b1xxxxxxxx"

producer_events = EventHubProducerClient.from_connection_string(conn_str=eventHubConnString, eventhub_name=eventHubNameevents)

hostname = socket.gethostname()

hostname = socket.gethostname()
from faker import Faker
from enum import Enum
import datetime


class EVENT_TYPE(Enum):
    CLICK = 1
    IMPRESSION = 2

productIds = [707,708,711,712,714,715,716,717,718,722,738,739,742,743,747,748,779,780,781,782,783,784,792,793,794,795,796,797,798,799,800,801,808,809,810,813,822,835,836,838,858,859,860,864,865,867,868,869,870,873,874,875,876,877,880,881,883,884,885,886,889,891,892,893,894,895,896,899,900,904,905,907,908,909,910,913,916,917,918,920,924,925,926,935,936,937,938,939,940,944,945,947,948,949,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,996,997,998,999]


def generateImpressionEvent(isAnomaly, productId):
    faker = Faker()

    event = {}
    event["eventType"] = EVENT_TYPE.IMPRESSION.name
    event["eventID"] = faker.uuid4()

    start_date = datetime.date(year=2024, month=1, day=1)
    end_date = datetime.date(year=2024, month=5, day=1)

    event["eventDate"] = faker.date_time_between_dates(start_date, end_date).isoformat()

    if productId:
        event["productId"] = str(productId)
    else:
        event["productId"] = faker.random_element(productIds)

    randomizeUnsupported = randrange(100)
    userAgent = {}
    userAgent["platform"] = faker.random_element(["Windows", "Mac", "Linux", "iOS", "Android"])
    if randomizeUnsupported <2:
        userAgent["browser"] = "Unsupported"
    else:
        if userAgent["platform"] == "Windows":
            userAgent["browser"] = faker.random_element(["Edge", "Chrome", "Firefox", "Safari"])
        elif userAgent["platform"] == "Mac":
            userAgent["browser"] = faker.random_element(["Chrome", "Firefox", "Safari"])
        elif userAgent["platform"] == "Linux":
            userAgent["browser"] = faker.random_element(["Chrome", "Firefox"])
        elif userAgent["platform"] == "iOS":
            userAgent["browser"] = faker.random_element(["Safari", "Chrome"])
        elif userAgent["platform"] == "Android":
            userAgent["browser"] = faker.random_element(["Chrome", "Firefox"])
        userAgent["browserVersion"] = faker.random_element(["10.2", "13.6", "8.6", "8.5", "11.2", "14.6", "6.6", "4.5"])
    event["userAgent"] = userAgent

    event["device"] = faker.random_element(["mobile", "computer", "tablet", "mobile", "computer"])
    event["ip_address"] = faker.ipv4()

    # only set the referer for CLICK events
    refererPayload = {}
    event["referer"] = refererPayload
    
    if isAnomaly:
        event["page_loading_seconds"] = faker.random_number(4)/100
    else:
        event["page_loading_seconds"] = faker.random_number(4)/1000
    return event


def generateClickEvent(impressionEvent, isAnomaly):
    faker = Faker()

    event = {}
    event["eventType"] = EVENT_TYPE.CLICK.name
    event["eventID"] = impressionEvent["eventID"]
    event["eventDate"] = impressionEvent["eventDate"]
    event["productId"] = impressionEvent["productId"]
    event["userAgent"] = impressionEvent["userAgent"]
    event["device"] = impressionEvent["device"]
    event["ip_address"] = impressionEvent["ip_address"]

    # only set the referer for CLICK events, refererPayload differs by campaign type
    refererPayload = {}    
    refererPayload["url"] = faker.uri()
    refererPayload["campaignType"] = faker.random_element( ["organic", "bing", "google", "facebook", "instagram", "twitter", "pinterest", "email", "affiliate"])
    match refererPayload["campaignType"]:
        case "bing"| "google"| "facebook"| "instagram":
            refererPayload["medium"] = "cpc"
            refererPayload["adId"] = faker.uuid4()
            refererPayload["adGroup"] = faker.uuid4()
            refererPayload["adTitle"] = faker.sentence()
        case "twitter":
            refererPayload["medium"] = "cpc"
            refererPayload["adId"] = faker.uuid4()
        case "pinterest":
            refererPayload["medium"] = "cpc"
            refererPayload["adId"] = faker.uuid4()
        case "email":
            refererPayload["medium"] = "email"
            refererPayload["campaignId"] = faker.uuid4()
            refererPayload["emailId"] = faker.email()
        case "affiliate":
            refererPayload["medium"] = "affiliate"
            refererPayload["affiliateId"] = faker.uuid4()
    event["referer"] = refererPayload
    
    if isAnomaly:
        event["page_loading_seconds"] = faker.random_number(4)/100
    else:
        event["page_loading_seconds"] = faker.random_number(4)/1000
    return event

import datetime
print(datetime.datetime.now())
generateEvents(False, None)
print(datetime.datetime.now())
```

And the below code I've developed for the SalesOrderHeaderDetail so that there are two eventstreaming data source 

```python
import json
import random
from faker import Faker
from datetime import datetime
from azure.eventhub import EventHubProducerClient, EventData

# Initialize Faker
faker = Faker()

# Azure Event Hub configuration
connection_string = 'Endpoint=sb://esehdewcdlgy5d4714h4zbya.servicebus.windows.net/;SharedAccessKeyName=key_xxxxxxxxx-18d4-bf13-11d9565eb9ed;SharedAccessKey=xweutSVMxxxxxxxqYaiqZ+AEhFubtQo=;EntityPath=es_c8954a72-1355-40b1-8a9a-b4cb9f53ebb4'  # Replace with actual connection string
event_hub_name = 'es_xxxxxxxxxx55-40b1-8a9a-b4cb9f53ebb4' 

# Function to generate Sales Order Event Data
def generate_sales_order_event():
    event = {}
    
    # Generate unique numeric IDs
    SalesOrderID = random.randint(1, 1000000)  # Random Sales Order ID between 1 and 1,000,000
    SalesOrderDetailID = random.randint(1, 1000000)  # Random Sales Order Detail ID between 1 and 1,000,000
    
    # Generate random quantities and prices
    OrderQty = random.randint(1, 100)  # Random quantity between 1 and 100
    ProductID = faker.random_int(min=700, max=999)  # Assuming product IDs are between 700 and 999
    UnitPrice = round(random.uniform(10.0, 500.0), 2)  # Random price between $10.00 and $500.00
    LineTotal = round(OrderQty * UnitPrice, 2)  # Calculate line total
    
    # Generate a modified date within the last year
    ModifiedDate = faker.date_time_this_year().isoformat()  # Random date this year in ISO format

    # Populate the event dictionary with generated data
    event["SalesOrderID"] = SalesOrderID   # Numeric ID
    event["SalesOrderDetailID"] = SalesOrderDetailID   # Numeric ID
    event["OrderQty"] = OrderQty
    event["ProductID"] = str(ProductID)
    event["UnitPrice"] = UnitPrice
    event["LineTotal"] = LineTotal
    event["ModifiedDate"] = ModifiedDate

    return event

# Function to send events to Azure Event Hub
def send_events_to_event_hub(events):
    producer = EventHubProducerClient.from_connection_string(conn_str=connection_string, eventhub_name=event_hub_name)
    
    with producer:
        for event in events:
            event_data = EventData(json.dumps(event))
            producer.send_event(event_data)
            print(f"Sent: {event}")


if __name__ == "__main__":
    print(datetime.now())
    
    # Generate a list of sales order events
    sales_order_events = [generate_sales_order_event() for _ in range(100)]
    
    # Send generated sales order events to Azure Event Hub
    send_events_to_event_hub(sales_order_events)
    
    print(datetime.now())
```

# Define Destination in Event Stream

After creating streaming data put into the destination i.e KQL DB we have two options before ingesting **Direct Ingestion** and **Event processing before ingestion** if we need any transformation in then data we can go with the  **Event processing before ingestion**

![AltImage](https://github.com/Jignesh3006/Streamlined-Data-Medallion-Architecture-for-Enhanced-Analytics/blob/main/Images/Step%20-%208.png)

![AltImage](https://github.com/Jignesh3006/Streamlined-Data-Medallion-Architecture-for-Enhanced-Analytics/blob/main/Images/Step%20-%209.png)

# Build the KQL db schema

In this section we create the table in the EventHouse and we will do the batch data ingestion using the Copy Data Activity before that we create the table. Below is all the query to create the data and simply run this query to create the data

``` KQL

// Bronze Layer
.create table [Address] (AddressID:int,AddressLine1:string,AddressLine2:string,City: string, StateProvince:string, CountryRegion:string, PostalCode: string, rowguid: guid, ModifiedDate:datetime)
.create table [Customer](CustomerID:int, NameStyle: string, Title: string, FirstName: string, MiddleName: string, LastName: string,Suffix:string, CompanyName: string, SalesPerson: string, EmailAddress: string, Phone: string, ModifiedDate: datetime)
.create table [SalesOrderHeader](SalesOrderID: int, OrderDate: datetime, DueDate: datetime, ShipDate: datetime, ShipToAddressID: int, BillToAddressID: int, SubTotal: decimal, TaxAmt: decimal, Freight: decimal, TotalDue: decimal, ModifiedDate: datetime)
.create table [SalesOrderDetail](SalesOrderID: int, SalesOrderDetailID: int, OrderQty: int, ProductID: int, UnitPrice: decimal , UnitPriceDiscount: decimal,LineTotal: decimal, ModifiedDate: datetime)
//adds a hidden field showing ingestion time
.alter table Address policy ingestiontime true
.alter table Customer policy ingestiontime true
.alter table SalesOrderHeader policy ingestiontime true
.alter table SalesOrderDetail policy ingestiontime true

//SILVER LAYER
.create table [SilverAddress] (AddressID:int,AddressLine1:string,AddressLine2:string,City: string, StateProvince:string, CountryRegion:string, PostalCode: string, rowguid: guid, ModifiedDate:datetime, IngestionDate: datetime)
.create table [SilverCustomer](CustomerID:int, NameStyle: string, Title: string, FirstName: string, MiddleName: string, LastName: string,Suffix:string, CompanyName: string, SalesPerson: string, EmailAddress: string, Phone: string, ModifiedDate: datetime, IngestionDate: datetime)
.create table [SilverSalesOrderHeader](SalesOrderID: int, OrderDate: datetime, DueDate: datetime, ShipDate: datetime, ShipToAddressID: int, BillToAddressID: int, SubTotal: decimal, TaxAmt: decimal, Freight: decimal, TotalDue: decimal, ModifiedDate: datetime, DaysShipped: long, IngestionDate: datetime)
.create table [SilverSalesOrderDetail](SalesOrderID: int, SalesOrderDetailID: int, OrderQty: int, ProductID: int, UnitPrice: decimal, UnitPriceDiscount: decimal,LineTotal: decimal, ModifiedDate: datetime, IngestionDate: datetime)

//GOLD LAYER
// use materialized views to view the latest changes in the orders table
.create materialized-view with (backfill=true) GoldAddress on table SilverAddress
{
    SilverAddress
    | summarize arg_max(IngestionDate, *) by AddressID
}

.create  materialized-view with (backfill=true) GoldDailyClicks on table Event
{
  Event
    | where eventType == "CLICK"
    | extend dateOnly = substring(todatetime(eventDate).tostring(), 0, 10) 
    | summarize count() by dateOnly, eventType
}
.create  materialized-view with (backfill=true) GoldDailyImpressions on table Event
{
  Event
    | where eventType == "IMPRESSION"
    | extend dateOnly = substring(todatetime(eventDate).tostring(), 0, 10) 
    | summarize count() by dateOnly, eventType
}

```
# Data Pipeline

Using CopyData Activity in the Pipeline will do the batch ingest and to ensure it to keep the data consistent. This ingestion  is one time ingestion and we can schedule the pipeline

![AltImage](https://github.com/Jignesh3006/Streamlined-Data-Medallion-Architecture-for-Enhanced-Analytics/blob/main/Images/Step%20-%2010.png)

After configuring the source and destination 

![AltImage](https://github.com/Jignesh3006/Streamlined-Data-Medallion-Architecture-for-Enhanced-Analytics/blob/main/Images/Step%20-%2011.png)

Configuring the destination i.e KQL DB in the Event House

![AltImage](https://github.com/Jignesh3006/Streamlined-Data-Medallion-Architecture-for-Enhanced-Analytics/blob/main/Images/Step%20-%2012.png)

Mapping the dataset with the columns present in the source dataset with respect to the data present in the EventHouse

![AltImage](https://github.com/Jignesh3006/Streamlined-Data-Medallion-Architecture-for-Enhanced-Analytics/blob/main/Images/Step%20-%2013.png)

# Materialized View

Creating an aggregated data using the Table which is present in the silver layer of the table. 

![AltImage](https://github.com/Jignesh3006/Streamlined-Data-Medallion-Architecture-for-Enhanced-Analytics/blob/main/Images/Step%20-%2014.png)

# Real - Time Dashboard

Building a real time dashboard to visualise the streaming data.
Below is the query to create the dashboard

``` KQL
-- TotalClicks Over Time

DemoEvents
| where eventType == "CLICK"
| summarize TotalClicks = count() by bin(eventDate, 1d)
| render timechart

-- Average Page Load Time

DemoEvents
| where eventDate   between (datetime(2024-01-01)..datetime(2024-06-30)) and eventType == "IMPRESSION" 
| summarize average_loadtime = avg(page_loading_seconds) by bin(eventDate, 1d) 
| render linechart 

-- TotalImpressions over Time
DemoEvents
| where eventType == "IMPRESSION"
| summarize TotalImpressions = count() by bin(eventDate, 1d)
| render timechart

-- Average Page Load Time Per Device Type
DemoEvents
| summarize AvgLoadTime = avg(page_loading_seconds) by device
| render piechart

```


![AltImage](https://github.com/Jignesh3006/Streamlined-Data-Medallion-Architecture-for-Enhanced-Analytics/blob/main/Images/Step%20-%2015.png)





## Getting Started
### Prerequisites
- Microsoft Azure account
- Microsoft Fabric subscription
- Access to Azure Synapse or related tools

### Installation
Instructions on how to set up the pipeline and run the validations can be found in the associated documentation. Ensure to configure your lakehouse paths and metadata files correctly.








